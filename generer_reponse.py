import json
import re
from typing import Dict, List, Any, Optional
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.schema.output_parser import StrOutputParser
from langchain.schema import BaseOutputParser
import logging

# --- Configuration ---
CLIENT_ID = "bms_ventouse"
CLIENT_DATA_FILE = f"./clients/{CLIENT_ID}/data.json"
CHROMA_COLLECTION_NAME = CLIENT_ID
CHROMA_DB_DIRECTORY = "./chroma_db"

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AdvancedOutputParser(BaseOutputParser):
    """Parser avancÃ© pour nettoyer et formater les rÃ©ponses"""
    
    def parse(self, text: str) -> str:
        # Nettoyage des artefacts de gÃ©nÃ©ration
        cleaned_text = re.sub(r'\[.*?\]', '', text)
        cleaned_text = re.sub(r'\*+\s?', '', cleaned_text)
        cleaned_text = re.sub(r'\n+', '\n', cleaned_text)
        
        # Suppression du prompt si prÃ©sent dans la rÃ©ponse (CORRECTION CRITIQUE)
        if "# ğŸ¬ MISSION" in cleaned_text or "VOCABULAIRE" in cleaned_text:
            # Le modÃ¨le a renvoyÃ© le prompt - extraire seulement la rÃ©ponse
            parts = cleaned_text.split("**RÃ©ponse :**")
            if len(parts) > 1:
                cleaned_text = parts[-1].strip()
            else:
                # Chercher aprÃ¨s "RÃ©ponse:"
                parts = cleaned_text.split("RÃ©ponse:")
                if len(parts) > 1:
                    cleaned_text = parts[-1].strip()
        
        # Suppression des phrases rÃ©pÃ©tÃ©es
        sentences = cleaned_text.split('. ')
        unique_sentences = []
        for sentence in sentences:
            if sentence and sentence not in unique_sentences:
                unique_sentences.append(sentence)
        
        result = '. '.join(unique_sentences).strip()
        
        # Si la rÃ©ponse est trop courte ou contient encore du prompt, fallback
        if len(result) < 30 or "MISSION" in result or "VOCABULAIRE" in result:
            return "Merci pour votre message ! Notre Ã©quipe BMS Ventouse est Ã  votre disposition. Contactez-nous au 06 XX XX XX XX pour une rÃ©ponse personnalisÃ©e. ğŸ¬"
        
        return result

class ContextEnhancer:
    """AmÃ©liore le contexte rÃ©cupÃ©rÃ© avec des mÃ©tadonnÃ©es intelligentes"""
    
    def __init__(self, client_data: Dict):
        self.client_data = client_data
    
    def enhance_context(self, docs: List[Any]) -> str:
        """Enrichit le contexte avec des informations structurÃ©es"""
        if not docs:
            # FALLBACK CRITIQUE : Fournir un contexte de base quand aucun document n'est trouvÃ©
            return f"""
BMS Ventouse - Expert logistique audiovisuelle
Services: Ventousage vÃ©hicules, gestion stationnement plateau, rÃ©gie technique
Contact: Disponible 24/7 pour urgences tournage
RÃ©fÃ©rences: Netflix, Amazon Prime, grandes productions franÃ§aises
"""
        
        enhanced_parts = []
        for doc in docs[:3]:  # Limiter Ã  3 docs pour tinyllama
            source_type = doc.metadata.get('type', 'general')
            source_content = doc.page_content
            
            if source_type == 'reference_client':
                enhanced_parts.append(f"RÃ©fÃ©rence: {source_content}")
            elif source_type == 'offre_service':
                enhanced_parts.append(f"Service: {source_content}")
            elif source_type == 'gestion_crise':
                enhanced_parts.append(f"Urgent: {source_content}")
            else:
                enhanced_parts.append(source_content)
        
        return "\n".join(enhanced_parts)

class ResponseQualityChecker:
    """VÃ©rifie la qualitÃ© des rÃ©ponses gÃ©nÃ©rÃ©es"""
    
    @staticmethod
    def check_response_quality(response: str, min_length: int = 50) -> Dict[str, Any]:
        checks = {
            "has_signature": "BMS" in response or "Ventouse" in response,
            "sufficient_length": len(response) >= min_length,
            "has_contact_cta": any(keyword in response.lower() for keyword in 
                                 ['contact', 'appel', 'whatsapp', 'email', 'devis', 'disponible']),
            "no_prompt_leak": not any(leak in response for leak in 
                                     ["MISSION", "VOCABULAIRE", "DIRECTIVES", "# ğŸ¬"])
        }
        
        checks["all_passed"] = all(checks.values())
        return checks

def load_client_data() -> Dict[str, Any]:
    """Charge et valide les donnÃ©es client"""
    try:
        with open(CLIENT_DATA_FILE, 'r', encoding='utf-8') as f:
            client_data = json.load(f)
        logger.info(f"âœ… DonnÃ©es client chargÃ©es pour {client_data['entreprise']['nom']}")
        return client_data
    except Exception as e:
        logger.error(f"âŒ Erreur chargement donnÃ©es client: {e}")
        raise

def initialize_rag_system(client_data: Dict[str, Any]):
    """Initialise le systÃ¨me RAG complet"""
    
    # 1. Initialisation des modÃ¨les
    logger.info("Initialisation des modÃ¨les Ollama...")
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    llm = Ollama(
        model="tinyllama",
        temperature=0.7,  # Plus crÃ©atif pour compenser la petite taille
        num_predict=300,  # RÃ©ponses plus courtes = plus rapide
        top_k=20,
        top_p=0.9
    )
    
    # 2. Connexion Ã  la base vectorielle
    vectorstore = Chroma(
        collection_name=CHROMA_COLLECTION_NAME,
        embedding_function=embeddings,
        persist_directory=CHROMA_DB_DIRECTORY
    )
    
    # 3. Retrieveur avec seuil abaissÃ© (CORRECTION CRITIQUE)
    retriever = vectorstore.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={
            "k": 3,  # Seulement 3 docs pour tinyllama (moins de contexte)
            "score_threshold": 0.3  # ABAISSÃ‰ de 0.6 Ã  0.3 pour plus de rÃ©sultats
        }
    )
    
    # 4. Template de prompt SIMPLIFIÃ‰ pour tinyllama (CORRECTION CRITIQUE)
    template = """Tu es l'assistant de BMS Ventouse, expert en logistique audiovisuelle.

INFORMATIONS ENTREPRISE:
{context}

CLIENT DIT: "{question}"

SITUATION: {scenario_type}

Ta mission: RÃ©ponds en 2-3 phrases courtes et professionnelles. Propose une solution concrÃ¨te. Termine par un appel Ã  l'action (contact, devis, etc.).

RÃ©ponse professionnelle:"""
    
    # 5. Initialisation des composants avancÃ©s
    context_enhancer = ContextEnhancer(client_data)
    output_parser = AdvancedOutputParser()
    
    # 6. DÃ©tection de scÃ©nario simplifiÃ©e
    def detect_scenario(question: str) -> str:
        question_lower = question.lower()
        
        if any(kw in question_lower for kw in ['urgent', 'demain', 'crise']):
            return "URGENCE dÃ©tectÃ©e"
        elif any(kw in question_lower for kw in ['prix', 'devis', 'budget']):
            return "Demande de DEVIS"
        elif any(kw in question_lower for kw in ['rÃ©fÃ©rence', 'expÃ©rience']):
            return "Demande de RÃ‰FÃ‰RENCES"
        else:
            return "Question gÃ©nÃ©rale"
    
    # 7. Construction du prompt
    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question", "scenario_type"]
    )
    
    # 8. ChaÃ®ne RAG SIMPLIFIÃ‰E (CORRECTION CRITIQUE)
    def process_query(question: str) -> str:
        try:
            # RÃ©cupÃ©ration du contexte
            docs = retriever.get_relevant_documents(question)
            context = context_enhancer.enhance_context(docs)
            scenario = detect_scenario(question)
            
            # Log pour debugging
            logger.info(f"ğŸ“„ Documents trouvÃ©s: {len(docs)}")
            logger.info(f"ğŸ¯ ScÃ©nario: {scenario}")
            
            # GÃ©nÃ©ration
            prompt_text = prompt.format(
                context=context,
                question=question,
                scenario_type=scenario
            )
            
            # Appel au LLM
            raw_response = llm.invoke(prompt_text)
            
            # Parsing
            parsed_response = output_parser.parse(raw_response)
            
            return parsed_response
            
        except Exception as e:
            logger.error(f"Erreur traitement: {e}")
            return "Merci pour votre message ! Contactez BMS Ventouse pour une rÃ©ponse personnalisÃ©e. ğŸ¬"
    
    return process_query, vectorstore

def main():
    """Fonction principale avec interface utilisateur amÃ©liorÃ©e"""
    
    try:
        # Chargement des donnÃ©es
        client_data = load_client_data()
        
        # Initialisation du systÃ¨me RAG
        process_query, vectorstore = initialize_rag_system(client_data)
        
        # VÃ©rification du nombre de documents
        collection_count = vectorstore._collection.count()
        logger.info(f"ğŸ“Š Base vectorielle contenant {collection_count} documents")
        
        print(f"\n{'='*60}")
        print(f"ğŸ¤– CM-AI - Assistant {client_data['entreprise']['nom']}")
        print(f"ğŸ¯ {client_data['entreprise']['slogan']}")
        print(f"{'='*60}")
        print("ğŸ’¡ Exemples de questions Ã  tester :")
        print("   â€¢ 'Urgence pour tournage demain Ã  Paris'")
        print("   â€¢ 'Besoin devis pour ventousage sÃ©rie TV'")
        print("   â€¢ 'Vous avez des rÃ©fÃ©rences sur Netflix ?'")
        print("   â€¢ 'ProblÃ¨me autorisation mairie pour plateau'")
        print(f"{'='*60}")
        print("Tapez 'quitter' pour arrÃªter\n")
        
        quality_checker = ResponseQualityChecker()
        
        while True:
            try:
                avis_client = input("\nğŸ¬ Commentaire client : ").strip()
                
                if avis_client.lower() in ['quitter', 'exit', 'quit']:
                    break
                    
                if not avis_client:
                    continue
                
                print("\nğŸ§  Analyse en cours...")
                
                # GÃ©nÃ©ration de la rÃ©ponse
                response = process_query(avis_client)
                
                # VÃ©rification qualitÃ©
                quality_report = quality_checker.check_response_quality(response)
                
                # Affichage des rÃ©sultats
                print(f"\nâœ… RÃ‰PONSE GÃ‰NÃ‰RÃ‰E :")
                print(f"{'â”€'*50}")
                print(response)
                print(f"{'â”€'*50}")
                
                # Rapport qualitÃ©
                if not quality_report["all_passed"]:
                    logger.warning("âš ï¸  RÃ©ponse sous-optimale dÃ©tectÃ©e")
                    logger.debug(f"Quality checks: {quality_report}")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ArrÃªt demandÃ©. Au revoir !")
                break
            except Exception as e:
                logger.error(f"âŒ Erreur lors du traitement: {e}")
                print("âŒ DÃ©solÃ©, une erreur s'est produite. Veuillez rÃ©essayer.")
                
    except Exception as e:
        logger.error(f"âŒ Erreur initialisation: {e}")
        print("âŒ Impossible de dÃ©marrer l'assistant. VÃ©rifiez la configuration.")

if __name__ == "__main__":
    main()
